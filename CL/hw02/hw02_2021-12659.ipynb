{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이름 : 박유나\n",
    "학번 : 2021-12659 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파일 및 모듈 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2024-05-13 03:56:48--  https://www.manythings.org/anki/spa-eng.zip\n",
      "Resolving www.manythings.org... 173.254.30.110\n",
      "Connecting to www.manythings.org|173.254.30.110|:443... connected.\n",
      "OpenSSL: error:1407742E:SSL routines:SSL23_GET_SERVER_HELLO:tlsv1 alert protocol version\n",
      "Unable to establish SSL connection.\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.manythings.org/anki/spa-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip spa-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x122cd5ddb90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchtext\n",
    "import torch\n",
    "SEED = 123\n",
    "\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YUNA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchtext\\data\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "c:\\Users\\YUNA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchtext\\vocab\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "c:\\Users\\YUNA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchtext\\utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import time\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(fname):\n",
    "    list_iter = []\n",
    "    with open(fname, 'r', encoding='UTF8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[1:]:\n",
    "            eng, spa, attribut = line.strip().split('\\t')\n",
    "            list_iter.append([eng, spa])\n",
    "\n",
    "    return list_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data sampling and splitting\n",
    "data_iter = read_file('spa.txt')\n",
    "data_iter = random.sample(data_iter, 10000)\n",
    "dataset = to_map_style_dataset(data_iter)\n",
    "num_train = int(len(dataset) * 0.9) \n",
    "train_dataset, test_dataset = random_split(dataset, [num_train, len(dataset) - num_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컴퓨터 성능이 좋지 않아 10000개를 랜덤 추출하여 진행하였습니다. 데이터를 훈련과 테스트 데이터로 9:1의 비율로 나누었습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "# pre_trained tokenizer사용\n",
    "import spacy\n",
    "import spacy.cli.download\n",
    "spacy.cli.download('es_core_news_sm')\n",
    "spacy.cli.download('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "sp_nlp = spacy.load('es_core_news_sm')\n",
    "en_nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def en_yield_tokens(data_iter):\n",
    "    for en, _ in data_iter:\n",
    "        yield [token.text for token in en_nlp.tokenizer(en.lower())]\n",
    "\n",
    "def sp_yield_tokens(data_iter):\n",
    "    for _, sp in data_iter:\n",
    "        yield [token.text for token in sp_nlp.tokenizer(sp.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab 만들기\n",
    "en_voc = build_vocab_from_iterator(en_yield_tokens(train_dataset),\n",
    "                                specials=[\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"],\n",
    "                                max_tokens= 25000)\n",
    "sp_voc = build_vocab_from_iterator(sp_yield_tokens(train_dataset),\n",
    "                                specials=[\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"], #토큰들 추가\n",
    "                                max_tokens= 25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_token2id = en_voc.get_stoi()\n",
    "sp_token2id = sp_voc.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_pipeline = lambda x: [en_token2id.get(token.text, en_token2id['<unk>']) for token in en_nlp.tokenizer(x)]\n",
    "sp_pipeline = lambda x: [sp_token2id['<sos>']] + [sp_token2id.get(token.text, sp_token2id['<unk>']) for token in sp_nlp.tokenizer(x)] + [sp_token2id['<eos>']]\n",
    "# target sentence에 sos와 eos 토큰 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    en_list, sp_list= [], []\n",
    "    for en, sp in batch:\n",
    "        processed_en = torch.tensor(en_pipeline(en), dtype=torch.int64)\n",
    "        en_list.append(processed_en)\n",
    "        processed_sp = torch.tensor(sp_pipeline(sp), dtype=torch.int64)\n",
    "        sp_list.append(processed_sp)\n",
    "\n",
    "    en_list = pad_sequence(en_list, padding_value = 1) #padding\n",
    "    sp_list = pad_sequence(sp_list, padding_value = 1)\n",
    "    return en_list, sp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader 만들기\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16,\n",
    "                              shuffle=True, collate_fn=custom_collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16,\n",
    "                              shuffle=True, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hid_dim = hid_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim) # gru 사용\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, ouptput_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = ouptput_dim\n",
    "        self.hid_dim = hid_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(ouptput_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim)\n",
    "\n",
    "        self.fc_out = nn.Linear(emb_dim + hid_dim *2, ouptput_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, context):\n",
    "        input = input.unsqueeze(0)\n",
    "\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "\n",
    "        emb_con = torch.cat((embedded, context), dim =2)\n",
    "        \n",
    "        output, hidden = self.rnn(emb_con, hidden)\n",
    "\n",
    "        output = torch.cat((embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)), dim=1)\n",
    "\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        context = self.encoder(src)\n",
    "        hidden = context\n",
    "\n",
    "        input = trg[0, :]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input, hidden, context)\n",
    "\n",
    "            outputs[t] = output\n",
    "\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            topl = output.argmax(1)\n",
    "\n",
    "            input = trg[t] if teacher_force else topl\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default setting\n",
    "\n",
    "input_dim = len(en_voc.get_itos())\n",
    "output_dim = len(sp_voc.get_itos())\n",
    "enc_emb_dim = 256\n",
    "dec_emb_dim = 256\n",
    "hid_dim = 512\n",
    "n_layers = 2\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "enc = Encoder(input_dim, enc_emb_dim, hid_dim, enc_dropout)\n",
    "dec = Decoder(output_dim, dec_emb_dim, hid_dim, dec_dropout)\n",
    "\n",
    "model = Seq2seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(4707, 256)\n",
       "    (rnn): GRU(256, 512)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(7463, 256)\n",
       "    (rnn): GRU(768, 512)\n",
       "    (fc_out): Linear(in_features=1280, out_features=7463, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer와 loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) \n",
    "criterion = nn.CrossEntropyLoss(ignore_index=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련을 위한 함수\n",
    "def train(model, dataloader, optimizer, criterion, clip):\n",
    "  epoch_loss = 0\n",
    "\n",
    "  model.train() \n",
    "\n",
    "  for en, sp in dataloader:  \n",
    "    src = en.to(device)\n",
    "    trg = sp.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(src, trg)\n",
    "\n",
    "    output_dim = output.shape[-1]\n",
    "        \n",
    "    output = output[1:].view(-1, output_dim)\n",
    "    trg = trg[1:].view(-1)\n",
    "        \n",
    "        \n",
    "    loss = criterion(output, trg)\n",
    "\n",
    "    loss.backward()  \n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "    optimizer.step()\n",
    "        \n",
    "    epoch_loss += loss.item()\n",
    "\n",
    "  return epoch_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "  elapsed_time = end_time - start_time\n",
    "  elapsed_mins = int(elapsed_time / 60)\n",
    "  elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "  return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 7m 37s\n",
      "\tTrain Loss: 4.494\n"
     ]
    }
   ],
   "source": [
    "# 모델 훈련\n",
    "\n",
    "N_EPOCHS = 1\n",
    "clip = 1\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "  start_time = time.time()\n",
    "\n",
    "  train_loss= train(model, train_dataloader, optimizer, criterion, clip)\n",
    "\n",
    "  end_time = time.time()\n",
    "\n",
    "  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "  print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "  print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), 'hw02.pt') # model save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가함수\n",
    "def evaluate(model, dataloader, criterion):\n",
    "  epoch_loss = 0\n",
    "\n",
    "  model.eval() \n",
    "\n",
    "  with torch.no_grad():\n",
    "    for en, sp in dataloader:\n",
    "      src = en.to(device)\n",
    "      trg = sp.to(device)\n",
    "\n",
    "      output = model(src, trg)\n",
    "      output_dim = output.shape[-1]\n",
    "        \n",
    "      output = output[1:].view(-1, output_dim)\n",
    "      trg = trg[1:].view(-1)\n",
    "        \n",
    "        \n",
    "      loss = criterion(output, trg)\n",
    "        \n",
    "      epoch_loss += loss.item()\n",
    "\n",
    "  return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_id2token = sp_voc.get_itos()\n",
    "\n",
    "en_token2id = en_voc.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 문장을 번역하는 함수\n",
    "def translate_sentence(model, sentence, src_field, trg_field, device, max_len=50, logging=True):\n",
    "    model.eval() \n",
    "\n",
    "    tokens = [token.text for token in en_nlp.tokenizer(sentence.lower())]\n",
    "\n",
    "    tokens = ['<sos>'] + tokens + ['<eos>']\n",
    "\n",
    "    src_indexes = [en_token2id.get(token, en_token2id['<unk>']) for token in tokens]\n",
    "\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        context = model.encoder(src_tensor)\n",
    "        hidden = context\n",
    "\n",
    "    trg_indexes = [sp_token2id['<sos>']]\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model.decoder(trg_tensor, hidden, context)\n",
    "            pred_token = output.argmax().item()\n",
    "\n",
    "        \n",
    "        trg_indexes.append(pred_token) \n",
    "\n",
    "        if pred_token == sp_token2id['<eos>']:\n",
    "            break\n",
    "\n",
    "    trg_tokens = [sp_id2token[i] for i in trg_indexes]\n",
    "\n",
    "    return trg_tokens[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bleu score를 계산하는 함수\n",
    "def bleu(dataset, model, SRC, TRG, device):\n",
    "    targets = []\n",
    "    outputs = []\n",
    "    bleu_sc = []\n",
    "\n",
    "    for en, sp in dataset:\n",
    "\n",
    "      prediction = translate_sentence( model, en, SRC, TRG, device)\n",
    "      prediction = prediction[:-1]  # remove <eos> token\n",
    "      outputs.append(prediction)\n",
    "\n",
    "      targets.append([[token.text for token in en_nlp.tokenizer(sp)]])\n",
    "\n",
    "    bleu_sc.append(bleu_score(outputs, targets))\n",
    "\n",
    "    return bleu_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 3.617 | Bleu Score: [0.00906093418598175]\n"
     ]
    }
   ],
   "source": [
    "# test loss와 bleu score 계산\n",
    "#model = model.load_state_dict(torch.load('hw02.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_dataloader, criterion)\n",
    "bleu_sc = bleu(test_dataset, model, en_voc, sp_voc, device)\n",
    "print(f'Test Loss: {test_loss:.3f} | Bleu Score: {bleu_sc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test sentence: He is a baseball player .\n",
      "model output: <unk> es es un un . . <eos>\n"
     ]
    }
   ],
   "source": [
    "# test sentence 번역\n",
    "src = 'He is a baseball player .'\n",
    "\n",
    "print(f'test sentence: {src}')\n",
    "print(\"model output:\", \" \".join(translate_sentence(model, src, en_voc, sp_voc, device)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "블루스코어는 매우 낮고, test loss는 높다. 하지만 test sentence에 대해서 '<unk>(남성 : es)는 축구선수이다.'로 번역한 것으로 보아 그럭저럭 비슷하게 잘 번역하였다.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
