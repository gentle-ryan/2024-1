---
title: "Lecture 3 R Practice"
author: "Sangjin Lee"
date: "2024년 4월 1일"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ggplot2)
library(dplyr)
# install.packages("agricolae")
library(agricolae)
library(DescTools)
```

### (1) Permuational F-test

#### $F_i(x) = F(x-\mu_i), \ i=1, ..., K$

#### $H_0: F_1(x) = ... = F_K(x)$

#### Example: 주말 오후 1-3시에 서울 잠실 일대 공유 전동킥보드 세 브랜드(킥고잉, 라임, 고고씽)의 이용 시간을 조사하였다.

```{r}
kick <- read.csv("kickboard.csv")
head(kick)
```

```{r}
group_by(kick, brand) %>%
  summarise(count=n(), mean=mean(time), sd=sd(time))
```

```{r}
ggplot(kick, aes(x=time, color=brand, fill=brand)) +
  geom_density(alpha=0.5) + geom_rug()
```

```{r}
summary(aov(time ~ brand, data=kick))
```

```{r}
N <- nrow(kick); K <- 3

group.length <- aggregate(kick$time, by=list(kick$brand), length)[,2]
group.mean <- aggregate(kick$time, by=list(kick$brand), mean)[,2]

SSTotal <- var(kick$time) * (length(kick$time)-1)
SST <- sum(group.length * (group.mean - mean(kick$time))^2)
SSE <- SSTotal - SST

F.obs <- (SST/(K-1))/(SSE/(N-K)); F.obs
```

```{r}
R = 999 # number of permutations
F.perm <- vector(length=R)
for (r in 1:R) {
  perm <- sample(1:N, replace=FALSE)
  group.length <- aggregate(kick$time, by=list(kick$brand[perm]), length)[,2]
  group.mean <- aggregate(kick$time, by=list(kick$brand[perm]), mean)[,2]
  
  SSTotal <- var(kick$time) * (length(kick$time)-1)
  SST <- sum(group.length * (group.mean - mean(kick$time))^2)
  SSE <- SSTotal - SST
  F.perm[r] <- (SST/(K-1))/(SSE/(N-K))
}

p.value <- (sum(F.perm>=F.obs)+1)/(R+1); p.value
```

```{r}
data.frame(F.perm=F.perm) %>% ggplot(aes(x=F.perm)) +
  geom_histogram(aes(y=..density..), binwidth=0.1, color="white") +
  geom_vline(xintercept=F.obs, color='red')
```

### (2) Kruskal-Wallis test: without ties

#### $F_i = F(x-\mu_i), \ i=1, …, K$

#### $H_0: F_1(x) = … = F_K(x)$

#### Example: 떡갈나무 16그루에 대해 연간 밑둥 직경의 증가량을 측정하였다. 각 나무는 위치한 부지의 경사도에 따라 분류되었다. (1: 경사 급함 \~ 5: 경사 없음)

```{r}
tree <- read.csv("tree.csv")
head(tree)
```

```{r}
ggplot(tree, aes(x=as.factor(slope), y=increment)) + geom_boxplot()
```

```{r}
kruskal.test(increment ~ slope, data=tree)
```

```{r}
tree.rank <- tree %>% mutate(rank=rank(increment))
head(tree.rank)
```

```{r}
group.length <- aggregate(rank ~ slope, data=tree.rank, length)[,2]
group.rank.mean <- aggregate(rank ~ slope, data=tree.rank, mean)[,2]
```

```{r}
N <- sum(group.length); K <- length(group.length)
KW.obs <- ( 12/(N*(N+1))) * sum( group.length * (group.rank.mean - (N+1)/2)^2 )
KW.obs
```

```{r}
p.value <- pchisq(KW.obs, df=K-1, lower.tail=F)
p.value
```

### (3) Jonckheere-Terpstra Test: ordered alternative

#### $H_a: F_i(x) \geq F_j(x)$ for $i < j$

```{r}
tree$slope <- factor(tree$slope, ordered=TRUE)
# exact test
DescTools::JonckheereTerpstraTest(increment ~ slope, data=tree, alternative="increasing")
```

### (4) Multiple testing: controlling FWER

#### $X_{l, 1}, …, X_{1, 100} \sim N(\mu_l,1), \ l=1, ..., 20$

#### $H_{0l}: \mu_{l}=0, \ l=, …, 20$

#### The following simulations shows that setting $\alpha=0.05$ for each test leads to undesired results.

```{r}
L=20 # number of tests
n=100 # size of samples
R=1000
err.vec <- vector(length=R)
for (r in 1:R) {
  fam.err <- FALSE
  x=matrix(rnorm(n*L), nrow=n)
  for (i in 1:L) {
    if (t.test(x[,i], mu=0)$p.value < 0.05) {
      fam.err <- TRUE
      break
    }
  }
  err.vec[r] <- fam.err
}

(fam.err.rate <- sum(err.vec)/R)
```

#### Given that $H_{0l}, \ l=1, ..., 20$ being all true,

#### FWER = Pr(at least one type 1 error in the family)

#### = 1 - Pr(no type 1 error in the family)

#### = $1- (1 - 0.05)^{20}$

```{r}
1 - (1-0.05)^20
```

#### Need to use a smaller significance level $\alpha\prime$ for each pairwise test!

### (5) Multiple comparisons: Bonferroni adjustment

#### Example: Percentages of clay in soil samples from four locations. (Populations are normally distributed with equal variances.)

```{r}
clay <- read.csv("clay.csv")
head(clay)
```

```{r}
group_by(clay, location) %>%
  summarise(count=n(), mean=mean(pct), sd=sd(pct))
```

```{r}
summary(aov(pct ~ as.factor(location), data=clay))
```

#### Bonferroni adjustment is to do each of the $K(K-1)/2=6$ comparisons at level of significance $\alpha\prime = \alpha/6$.

```{r}
pct.all <- matrix( c(clay$pct[clay$location==1], 
                     clay$pct[clay$location==2],
                     clay$pct[clay$location==3],
                     clay$pct[clay$location==4]), 
                   byrow=FALSE, ncol=4 )

pvals <- matrix(, nrow=4, ncol=4)
for (i in 1:4) {
  j <- i+1
  while(j<=4) {
    pvals[i,j] <- t.test(pct.all[,i], pct.all[,j], var.equal=TRUE)$p.value
    j <- j+1}
}

pvals # raw p-values
```

```{r}
pvals[is.na(pvals)==FALSE] <- p.adjust(pvals[is.na(pvals)==FALSE], method="bonferroni")
```

```{r}
pvals # Bonferroni-adjusted p-values
```

```{r}
t.test(clay$pct[clay$location==2], clay$pct[clay$location==4], var.equal=TRUE)$p.value*6
```

```{r}
(bonf.critical.value <- qt(0.025/6, df=2*6-2))
```

### (6) Multiple comparisons: Fisher's protected LSD

#### Fisher's LSD procedure is to declare means for treatments $i$ and $j$ to be different if

#### $|\bar{X}_i - \bar{X}_j| \geq t(\alpha/2, df=N-K) \sqrt{MSE(2/6)}$

```{r}
N <- nrow(clay); K <-4
clay$location <- as.factor(clay$location)
model <- aov(pct ~ location, data=clay)

MSE <- deviance(model)/df.residual(model)

(lsd.critical.value <- qt(0.025, df=N-K) * sqrt(MSE*(2/6)))
```

```{r}
(agricolae::LSD.test(model, "location", p.adj="none"))
```

### (7) Multiple comprisons: Tukey's HSD

#### Tukey's HSD procedure considers a statistic that measures the largest difference between sample means:

#### $Q=\max_{ij} \{ \frac{\sqrt{n}|\bar{X}_i-\bar{X}_j|}{\sqrt{MSE}} \} = \frac{\sqrt{n} (\max_{i}\bar{X}_i- \min_{i} \bar{X}_i)}{\sqrt{MSE}}.$

#### The procedure declares means for treatment $i$ and $j$ to be different if

#### $|\bar{X}_i-\bar{X}_j|/\sqrt{MSE/n} \geq q(\alpha, K, df=N-k)$

#### where $q(\alpha, K, df=N-k)$ is the upper-tail $100\alpha$% point of the distribution of $Q$.

```{r}
(out <- TukeyHSD(model, "location", ordered=FALSE))
```

```{r}
Tij.obs <- abs(out$location[,1]); Tij.obs
```

```{r}
R = 2000
Q.perm <- vector(length=R)
for (r in 1:R) {
  clay.perm <- data.frame(pct = clay$pct, location=clay$location[sample(1:N, replace=FALSE)])
  model <- aov(pct ~ location, data=clay.perm)
  MSE <- deviance(model)/df.residual(model)
  group.mean <- aggregate(pct ~ location, data=clay.perm, mean)[,2]
  Tij <- abs(outer(group.mean, group.mean, "-"))/ sqrt(MSE/6)
  Q.perm[r] <- max(Tij)
} 
```

```{r}
( p.values <- unlist(purrr::map(Tij.obs, function(x) {sum(Q.perm>=x)/R})) )
```

```{r}
data.frame(Q.perm=Q.perm) %>% ggplot(aes(Q.perm)) +
  geom_histogram(aes(y=..density..), binwidth=0.1, color="white", alpha=0.6) +
  geom_vline(xintercept=Tij.obs, color="red") +
  geom_text(data=data.frame(x=Tij.obs, label=names(Tij.obs)),
            aes(x=x, y=0.1, label=label), color="red")
```
